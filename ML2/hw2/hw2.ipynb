{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = np.load(’/cat_images.npy’) # make sure these point to the correct directory\n",
    "dog_data = np.load(’/dog_images.npy’)\n",
    "# our images are 540x499 pixels\n",
    "img_rows, img_cols = 540, 499\n",
    "# create label vector (we’ll say 1 = Cat, 0 = Dog)\n",
    "y = np.concatenate((np.ones(cat_data.shape[0]), np.zeros(dog_data.shape[0])))\n",
    "# put the data together\n",
    "pet_data = np.concatenate((cat_data, dog_data))\n",
    "# split the data into training and test\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "pet_data, y, test_size=0.25, random_state=42\n",
    ")\n",
    "# Normalize image data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "# Reshape to add channel dimension for Keras\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Keras/TensorFlow modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras/tensorflow imports\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build enhanced CNN model with 2 convolutional layers and 2 hidden layers\n",
    "# Architecture:\n",
    "# - Conv2D(32 filters, 3x3 kernel) with padding\n",
    "# - MaxPooling2D\n",
    "# - Conv2D(64 filters, 3x3 kernel) \n",
    "# - MaxPooling2D\n",
    "# - Flatten\n",
    "# - Dense(128, ReLU)\n",
    "# - Dense(64, ReLU)\n",
    "# - Dense(1, Sigmoid) for binary classification\n",
    "\n",
    "inpx = Input(shape=(img_rows, img_cols, 1))\n",
    "\n",
    "# First convolutional layer with padding and ReLU activation\n",
    "conv_layer1 = Conv2D(1, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(inpx)\n",
    "pool_layer1 = MaxPooling2D(pool_size=(2, 2))(conv_layer1)\n",
    "\n",
    "# Second convolutional layer with ReLU activation\n",
    "conv_layer2 = Conv2D(1, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(pool_layer1)\n",
    "pool_layer2 = MaxPooling2D(pool_size=(2, 2))(conv_layer2)\n",
    "\n",
    "# Flatten the output from convolutional layers\n",
    "flat_layer = Flatten()(pool_layer2)\n",
    "\n",
    "# First hidden layer with ReLU activation\n",
    "hid_layer1 = Dense(128, activation='relu')(flat_layer)\n",
    "\n",
    "# Second hidden layer with ReLU activation\n",
    "hid_layer2 = Dense(64, activation='relu')(hid_layer1)\n",
    "\n",
    "# Output layer with Sigmoid for binary classification\n",
    "out_layer = Dense(1, activation='sigmoid')(hid_layer2)\n",
    "\n",
    "# Create the model\n",
    "model = Model([inpx], out_layer)\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer\n",
    "# Adam is more efficient than SGD and adapts the learning rate automatically\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=binary_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9255ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with optimizations:\n",
    "# - batch_size=32 for faster training\n",
    "# - epochs=20 for better convergence\n",
    "# - verbose=1 to see training progress\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=32,\n",
    "                    epochs=20, \n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ad9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and display confusion matrix\n",
    "preds = model.predict(x_test)\n",
    "y_pred = (preds.reshape(-1) >= 0.5).astype(int)\n",
    "y_true = y_test.astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "tbl = pd.crosstab(y_pred, y_true, rownames=['Predicted'], colnames=['Actual'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(tbl)\n",
    "print(f\"\\nTrue Negatives: {tbl.iloc[0, 0]}\")\n",
    "print(f\"False Positives: {tbl.iloc[1, 0]}\")\n",
    "print(f\"False Negatives: {tbl.iloc[0, 1]}\")\n",
    "print(f\"True Positives: {tbl.iloc[1, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def convert_image_to_grayscale_with_padding(image_path, target_width=None, target_height=None):\n",
    "    \"\"\"\n",
    "    Converts a single .jpg image to grayscale and optionally pads it to the target dimensions.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input .jpg image.\n",
    "        target_width (int, optional): Target width for padding. If None, keeps the original width.\n",
    "        target_height (int, optional): Target height for padding. If None, keeps the original height.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Grayscale image as a numpy array, with optional padding applied.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert to grayscale\n",
    "        grayscale_img = img.convert('L')\n",
    "        \n",
    "        # If target dimensions are provided, pad the image\n",
    "        if target_width and target_height:\n",
    "            padded_img = ImageOps.pad(grayscale_img, (target_width, target_height), color=0)\n",
    "        else:\n",
    "            padded_img = grayscale_img\n",
    "        \n",
    "        # Convert the image to a numpy array\n",
    "        grayscale_array = np.array(padded_img)\n",
    "\n",
    "    return grayscale_array\n",
    "\n",
    "test = convert_image_to_grayscale_with_padding('Lafayette.jpg', 499, 540)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e789f7",
   "metadata": {},
   "source": [
    "###Question 3 from the HW:\n",
    "\n",
    "Links: \n",
    "https://www.openslr.org/83/\n",
    "https://huggingface.co/datasets/ylacombe/english_dialects \n",
    "\n",
    "\n",
    "Description of the problem of interest: As someone who has spent time studying abroad in England and has visited various parts of the UK, I've always wondered how to tell apart different accents around the Kingdom. More specifically the regional ones in England itself. There are quite a few that I specifically mix up often, so I think using a CNN to classify UK specific accents from a speech dataset would be interesting and doable. All of the audio is spoken in english but in varying accents. \n",
    "\n",
    "In terms of ethical concerns, the main one could be the use of people's voice if they are not consenting to being recorded. I will say that the dataset I linked contains speech recorded by volunteers, but if we use audio that was recorded without consent that would lead to a ethical issue. I don't think there would be any ethical issues with using the results of the model. \n",
    "\n",
    "My intuition for why a CNN is more appropriate than the other models and techniques we've covered in this class is that the CNN can use spectrograms (time x frequency of audio) like images, which has a 2D structure. Similar to image data, the audio data would have spacial relationships that simpler models wouldn't be able to learn or use. Also, we need deep learning in this use case because the patterns in the data will be complex, requiring the use of multi-layer networks. Simpler models are not able to pick up on such complex relationships in the data. And the use of convolutional layers can break down the data into lower dimensional space which will help computational efficiency and help avoid having too may features (the scary curse of dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6398ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
