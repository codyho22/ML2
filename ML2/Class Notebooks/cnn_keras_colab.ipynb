{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YSz-nwEjYx0O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# keras contains the tensorflow package, often used for fitting NNets\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.losses import binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# by default, the mnist data set in keras is already broken up into\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "u_6MXKmkYzBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26270d6a-066b-48bd-911a-3cdb6c950057"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the data are stored in a 3D array\n",
        "# where the first number is the total number of images\n",
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maz1lqjvdKgt",
        "outputId": "0eb1fd05-578e-4fd8-d0c0-dfbf5abdbe2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each image in the mnist data is a number stored in a 28 by 28 pixel image\n",
        "img_rows, img_cols=28, 28\n",
        "\n",
        "# this code would usually be a bit unnecessary because the data are all black and white (there is only one channel)\n",
        "# but keras can be used for RGB (3 channel) images, and expects an extra dimension to the numpy array that describes\n",
        "# the number of channels. We'll add a (pedantic) 1 to the dimensionality of each image.\n",
        "# We'll also divide by 255 to do MinMaxScaling so that they are all between 0 and 1\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) / 255"
      ],
      "metadata": {
        "id": "x4y-COOXY-53"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EkJt6-gQHot",
        "outputId": "4225b187-cbf1-4f99-aca1-65bf07a31fe1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usually the mnist data is classified using all 10 digits\n",
        "# but we'll simplify it and just try to predict 6 vs. not 6\n",
        "y_6train = (y_train == 6).astype(int)\n",
        "y_6test = (y_test == 6).astype(int)"
      ],
      "metadata": {
        "id": "MBgpXKVHaQ9Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the test output\n",
        "y_6test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTBTn4VDZZHf",
        "outputId": "efc9bfb7-5919-4f3c-b553-27e0f0005000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grab our image plotter from the other notebook\n",
        "def plot_img(x, im_shape):\n",
        "    plt.imshow(x.reshape(im_shape), cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.gcf().set_size_inches(4, 4)"
      ],
      "metadata": {
        "id": "rqZAzHXrbAxg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this should be a 6\n",
        "#x_test[x_test.shape[0]-1]"
      ],
      "metadata": {
        "id": "4tRvSGECchX8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_img(x_test[x_test.shape[0]-1], im_shape=(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "1Iel5HdibjLn",
        "outputId": "f55615f0-bd60-482a-9c69-5d9de29495d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to build a CNN with one Convolutional Layer\n",
        "# (with a single 3 by 3 kernel with stride 1 and no activation (you could add an activation to the Convolutional Layer if you wanted))\n",
        "# and a single hidden layer with ReLU activation\n",
        "# and using the sigmoid activation for the output:\n",
        "inpx = Input(shape=(img_rows, img_cols, 1))\n",
        "\n",
        "# can also add padding in here with the padding = 'valid' (no padding) or 'same' (padding) option\n",
        "# check out the full range of options here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
        "con_layer = Conv2D(1, kernel_size=(3, 3), strides=1, activation=None)(inpx)\n",
        "\n",
        "# could add pooling to reduce the dimension further and speed up computation\n",
        "#pool_layer = MaxPooling2D(pool_size=(3, 3))(conv_layer)\n",
        "\n",
        "# need to flatten the images for the hidden layer/output\n",
        "flat_G = Flatten()(con_layer)\n",
        "\n",
        "# can decide how many hidden nodes to use\n",
        "hid_layer = Dense(250, activation='relu')(flat_G)\n",
        "\n",
        "# could add a second hidden layer\n",
        "#hid_layer2 = Dense(100, activation='tanh')(hid_layer)\n",
        "\n",
        "out_layer = Dense(1, activation='sigmoid')(hid_layer)\n",
        "\n",
        "# you can add way more in terms of layers/different activation functions/etc. to potentially make this work better!"
      ],
      "metadata": {
        "id": "KntWyP-6bqib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now to fit the model\n",
        "model = Model([inpx], out_layer)\n",
        "# note: there are different optimizers you can use (SGD is similar to the Gradient Descent we would do manually,\n",
        "# but there are some more efficient options, such as Adam (adaptive step-size): https://keras.io/api/optimizers/)\n",
        "# we also need to specify the objective/loss function. The keras version of log-loss (which you'll use for binary classification) is \"binary_crossentropy\".\n",
        "# there are plenty of others to choose based on the situation: https://keras.io/api/losses/\n",
        "model.compile(optimizer=SGD(),\n",
        "              loss=binary_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2Xn0Izk4fyYw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, fit the model (this may take some time depending on your parameter choices)\n",
        "# if you want to speed this up further, you can specify a smaller batch_size (a random subset of the data to work on during each epoch)\n",
        "# numbers are pretty distinguishable, so it shouldn't take too many epochs to get a decent fit\n",
        "model.fit(x_train, y_6train, epochs=10) #if you just want a progress bar (verbose=1) or to not display anything (verbose=0) the output will be cleaner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-6pmiiQg2xq",
        "outputId": "aea9a19a-1210-4409-b744-c510653009d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9514 - loss: 0.1433\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.9887 - loss: 0.0365\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9908 - loss: 0.0295\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9928 - loss: 0.0230\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9941 - loss: 0.0186\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.9954 - loss: 0.0141\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9958 - loss: 0.0125\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0114\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9969 - loss: 0.0101\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9972 - loss: 0.0088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e591f76f30>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking test set performance\n",
        "score = model.evaluate(x_test, y_6test, verbose=0)\n",
        "print('loss=', score[0])\n",
        "print('accuracy=', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgfe6MNLg_3d",
        "outputId": "ff38c8d4-0d67-43e6-9d87-c31c672a96cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 0.01371559128165245\n",
            "accuracy= 0.9955999851226807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seems good, but check HOW it's good\n",
        "import pandas as pd\n",
        "\n",
        "preds = model.predict(x_test)\n",
        "y_true = np.asarray(y_6test).ravel().astype(int)\n",
        "y_pred = (preds.reshape(-1) >= 0.5).astype(int)\n",
        "tbl = pd.crosstab(y_pred, y_true, rownames=['Predicted'], colnames=['Actual'])\n",
        "print(tbl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKaO18XZKqaM",
        "outputId": "40aa59fd-f9f1-4f90-8ed9-1b36f7ff0d6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Actual        0    1\n",
            "Predicted           \n",
            "0          9032   34\n",
            "1            10  924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print kernel weights of the convolutional layer\n",
        "conv_weights = model.layers[1].get_weights()[0]\n",
        "W = conv_weights[:,:,0,0]\n",
        "print(\"Convolutional kernel weights:\")\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcND24tFjFX2",
        "outputId": "25cabfcd-4533-4adc-8536-65b46b46f0ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolutional kernel weights:\n",
            "[[-0.00999276  0.09565605 -0.02722668]\n",
            " [-0.33895501 -0.79619735 -1.0563811 ]\n",
            " [-0.37598798 -0.8659836  -1.2493305 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grab the convMat function from cnn.ipynb to do the forward pass and then visualize what the recovered kernel is doing\n",
        "def convMat(X, W):\n",
        "    # get the dimensions of the kernel and input\n",
        "    k, _ = W.shape\n",
        "    p, _ = X.shape\n",
        "\n",
        "    # calculate the dimensions of the output matrix\n",
        "    q = p - k + 1\n",
        "    G = np.zeros((q, q))\n",
        "\n",
        "    # do the thing\n",
        "    for m in range(q):\n",
        "        for n in range(q):\n",
        "            submatrix = X[m:m+k, n:n+k]\n",
        "            G[m, n] = np.sum(W * submatrix)\n",
        "\n",
        "    return G\n",
        "\n",
        "G = convMat(x_test[x_test.shape[0]-1][:,:,0], W)\n",
        "\n",
        "# image isn't changing a lot in the convolutional layer\n",
        "plot_img(G, G.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "ZajBgugwjZeO",
        "outputId": "3cfaf84a-da07-4a2c-9bc0-11f59ee28672"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sr_JORaUpE3j"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}